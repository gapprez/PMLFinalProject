{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pyro GP tutorial used as starting point:\n",
    "## https://pyro.ai/examples/gp.html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "import arviz\n",
    "\n",
    "# Partition observations\n",
    "X = np.asarray([x / 29 for x in range(1, 31)])\n",
    "np.random.shuffle(X)\n",
    "Y = 6 * np.square(X) - np.square(np.sin(6 * np.pi * X)) - 5 * np.power(X, 4) + 3 / 2 + np.random.normal(0.0, 0.1, 30)\n",
    "Xtrain, Xtest, Ytrain, Ytest = torch.tensor(X[10:]), torch.tensor(X[:10]), torch.tensor(Y[10:]), torch.tensor(Y[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a suitable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We chose a GP regression model and the Matern 3/2 kernel. In this setup, we have three hyper-parameters.\n",
    "# I) The variance of the kernel, II) the lengthscale of the kernel, and III) the gaussian noise of the model.\n",
    "# We chose to let the gaussian noise be fixed and equal to the noise of our data, while keeping the variance\n",
    "# and lengthscale of the kernel variable. The prior distrubition we chose is a multivariate normal\n",
    "# distribution (i.e. we consider the variance and lengthscale as normally distributed), with mean and variance\n",
    "# based on what seems reasonable for the Matern 3/2 kernel, based on the lecture slides.\n",
    "\n",
    "# Pick prior distribution\n",
    "prior = dist.MultivariateNormal(torch.tensor([1.5,1]), torch.eye(2))\n",
    "\n",
    "# Define model\n",
    "def model(xs, ys, theta):\n",
    "    kernel = gp.kernels.Matern32(input_dim=1, variance=theta[0], lengthscale=theta[1])\n",
    "    return gp.models.GPRegression(xs, ys, kernel, noise=torch.tensor(0.01))\n",
    "\n",
    "# Computes log-likelihood\n",
    "def logLikelihood(xs, ys, theta=prior.sample()):\n",
    "    # solution based on lecture notes section 6.3\n",
    "    kernel = gp.kernels.Matern32(input_dim=1, variance=theta[0], lengthscale=theta[1])\n",
    "    t1 = 0.5 * torch.transpose(ys, 0, 0) * torch.linalg.inv(kernel.forward(xs)) * ys\n",
    "    t2 = 0.5 * torch.log(torch.linalg.det(kernel.forward(xs)))\n",
    "    t3 = 15.0 * torch.log(2 * torch.tensor(np.pi))\n",
    "    return - t1 - t2 - t3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 600/600 [00:00, 55823.57it/s, step size=1.00e+00, acc. prob=1.000]\n"
     ]
    }
   ],
   "source": [
    "# Model is GP model from pyro\n",
    "W = 100 # Number of warmup steps\n",
    "C = 1 # Number of chains\n",
    "S = 500 # Number of samples used in prediction\n",
    "\n",
    "gp_model_nuts = model(Xtrain, Ytrain, prior.sample())\n",
    "nuts_kernel = pyro.infer.NUTS(model, jit_compile=True)\n",
    "mcmc = pyro.infer.MCMC(nuts_kernel, S, num_chains=C, warmup_steps=W)\n",
    "mcmc.run(Xtrain, Ytrain, prior.sample())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking quality of samples using arviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vic/Desktop/Computer Science/2/PML/Final Project/PMLFinalProject/venv/lib/python3.11/site-packages/arviz/stats/stats.py:1354: UserWarning: Selecting first found group: sample_stats\n",
      "  warnings.warn(f\"Selecting first found group: {data.groups()[0]}\")\n",
      "arviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference data with groups:\n",
      "\t> sample_stats\n",
      "           mean   sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  \\\n",
      "diverging   0.0  0.0     0.0      0.0        0.0      0.0     500.0     500.0   \n",
      "\n",
      "           r_hat  \n",
      "diverging    NaN  \n"
     ]
    }
   ],
   "source": [
    "posterior_samples = mcmc.get_samples()\n",
    "data = arviz.from_pyro(mcmc)\n",
    "print(data)\n",
    "summary = arviz.summary(data)\n",
    "print(summary)\n",
    "arviz.plot_trace(data)\n",
    "plt.show()\n",
    "# Maybe use this: arviz.rcParams['plot.max_subplots'] = 18\n",
    "# arviz.plot_posterior(data, var_names=['w3', 'b3']) # TODO: Change var names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
