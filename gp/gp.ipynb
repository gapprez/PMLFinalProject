{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pyro GP tutorial used as starting point:\n",
    "## https://pyro.ai/examples/gp.html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "import arviz\n",
    "\n",
    "# Partition observations\n",
    "X = np.asarray([x / 29 for x in range(1, 31)])\n",
    "np.random.shuffle(X)\n",
    "Y = 6 * np.square(X) - np.square(np.sin(6 * np.pi * X)) - 5 * np.power(X, 4) + 3 / 2 + np.random.normal(0.0, 0.1, 30)\n",
    "Xtrain, Xtest, Ytrain, Ytest = X[10:], X[:10], Y[10:], Y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a suitable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We chose a GP regression model and the Matern 3/2 kernel. In this setup, we have three hyper-parameters.\n",
    "# I) The variance of the kernel, II) the lengthscale of the kernel, and III) the gaussian noise of the model.\n",
    "# We chose to let the gaussian noise be fixed and equal to the noise of our data, while keeping the variance\n",
    "# and lengthscale of the kernel variable. The prior distrubition we chose is a multivariate normal\n",
    "# distribution (i.e. we consider the variance and lengthscale as normally distributed), with mean and variance\n",
    "# based on what seems reasonable for the Matern 3/2 kernel, based on the lecture slides.\n",
    "\n",
    "# Define model\n",
    "def model(xs, ys, theta):\n",
    "    kernel = gp.kernels.Matern32(input_dim=1, variance=theta[0], lengthscale=theta[1])\n",
    "    return gp.models.GPRegression(torch.tensor(xs), torch.tensor(ys), kernel, noise=torch.tensor(0.01))\n",
    "\n",
    "# Pick prior distribution\n",
    "prior = dist.MultivariateNormal(torch.tensor([1.5,1]), torch.eye(2))\n",
    "\n",
    "# Computes log-likelihood: log p(y,theta|X)\n",
    "def logLikelihood(params):\n",
    "    # TODO\n",
    "    return 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is GP model from pyro\n",
    "W = 100 # Number of warmup steps\n",
    "C = 1 # Number of chains\n",
    "S = 500 # Number of samples used in prediction\n",
    "\n",
    "model = None # Should be GP model\n",
    "nuts_kernel = pyro.infer.NUTS(model, jit_compile=True)\n",
    "mcmc = pyro.infer.MCMC(nuts_kernel, S, num_chains=C, warmup_steps=W)\n",
    "mcmc.run(X, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking quality of samples using arviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = mcmc.get_samples()\n",
    "data = arviz.from_pyro(mcmc)\n",
    "summary = arviz.summary(data)\n",
    "print(summary)\n",
    "arviz.plot_trace(data)\n",
    "plt.show()\n",
    "# Maybe use this: arviz.rcParams['plot.max_subplots'] = 18\n",
    "arviz.plot_posterior(data, var_names=['w3', 'b3']) # TODO: Change var names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
