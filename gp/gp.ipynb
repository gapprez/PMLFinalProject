{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tb/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## Pyro GP tutorial used as starting point:\n",
    "## https://pyro.ai/examples/gp.html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "import arviz\n",
    "\n",
    "# Partition observations\n",
    "X = np.asarray([x / 29 for x in range(1, 31)])\n",
    "np.random.shuffle(X)\n",
    "Y = 6 * np.square(X) - np.square(np.sin(6 * np.pi * X)) - 5 * np.power(X, 4) + 3 / 2 + np.random.normal(0.0, 0.1, 30)\n",
    "Xtrain, Xtest, Ytrain, Ytest = torch.tensor(X[10:]), torch.tensor(X[:10]), torch.tensor(Y[10:]), torch.tensor(Y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a suitable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We chose a GP regression model and the Matern 3/2 kernel. In this setup, we have three hyper-parameters.\n",
    "# I) The variance of the kernel, II) the lengthscale of the kernel, and III) the gaussian noise of the model.\n",
    "# We chose to let the gaussian noise be fixed and equal to the noise of our data, while keeping the variance\n",
    "# and lengthscale of the kernel variable. The prior distrubition we chose is a multivariate normal\n",
    "# distribution (i.e. we consider the variance and lengthscale as normally distributed), with mean and variance\n",
    "# based on what seems reasonable for the Matern 3/2 kernel, based on the lecture slides.\n",
    "\n",
    "# Define kernel\n",
    "def kernel(theta):\n",
    "    return gp.kernels.Matern32(input_dim=1, variance=theta[0], lengthscale=theta[1])\n",
    "\n",
    "# Define model\n",
    "def model(xs, ys, kernel):\n",
    "    return gp.models.GPRegression(xs, ys, kernel, noise=torch.tensor(0.01))\n",
    "\n",
    "# Computes log-likelihood\n",
    "def logLikelihood(xs, ys, kernel, theta):\n",
    "    # See derivation in report\n",
    "    t1 = 0.5 * torch.transpose(ys, 0, 0) * torch.linalg.inv(kernel.forward(xs)) * ys\n",
    "    t2 = 0.5 * torch.log(torch.linalg.det(kernel.forward(xs)))\n",
    "    t3 = 15.0 * torch.log(2 * torch.tensor(np.pi))\n",
    "    return (- t1 - t2 - t3) * prior.log_prob(theta)\n",
    "\n",
    "# Pick prior distributions\n",
    "pyro.clear_param_store()\n",
    "some_theta = [torch.tensor(1.5),torch.tensor(1)]\n",
    "k = kernel(some_theta)\n",
    "gpr = model(Xtrain, Ytrain, k)\n",
    "gpr.kernel.lengthscale = pyro.nn.PyroSample(dist.LogNormal(0.0, 1.0))\n",
    "gpr.kernel.variance = pyro.nn.PyroSample(dist.LogNormal(0.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute posterior predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance = 1.216914415359497\n",
      "lengthscale = 0.7207109332084656\n",
      "noise = 0.21862062811851501\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(gpr.parameters(), lr=0.01)\n",
    "loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "losses = []\n",
    "num_steps = 2000\n",
    "for i in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(gpr.model, gpr.guide)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "gpr.set_mode(\"guide\")\n",
    "print(\"variance = {}\".format(gpr.kernel.variance))\n",
    "print(\"lengthscale = {}\".format(gpr.kernel.lengthscale))\n",
    "print(\"noise = {}\".format(gpr.noise))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is GP model from pyro\n",
    "W = 100 # Number of warmup steps\n",
    "C = 1 # Number of chains\n",
    "S = 500 # Number of samples used in prediction\n",
    "\n",
    "model = None # Should be GP model\n",
    "nuts_kernel = pyro.infer.NUTS(model, jit_compile=True)\n",
    "mcmc = pyro.infer.MCMC(nuts_kernel, S, num_chains=C, warmup_steps=W)\n",
    "mcmc.run(X, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking quality of samples using arviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = mcmc.get_samples()\n",
    "data = arviz.from_pyro(mcmc)\n",
    "summary = arviz.summary(data)\n",
    "print(summary)\n",
    "arviz.plot_trace(data)\n",
    "plt.show()\n",
    "# Maybe use this: arviz.rcParams['plot.max_subplots'] = 18\n",
    "arviz.plot_posterior(data, var_names=['w3', 'b3']) # TODO: Change var names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
